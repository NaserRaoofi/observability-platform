apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  name: prometheus-agent-slo
  namespace: observability
  labels:
    app.kubernetes.io/name: sloth
    app.kubernetes.io/part-of: observability-stack
    service: prometheus-agent
spec:
  service: "prometheus-agent"
  labels:
    team: "platform"
    component: "metrics-collection"
    environment: "production"
  slos:
    # Scrape success rate SLO
    - name: "scrape-success-rate"
      objective: 99.5
      description: "99.5% of metric scrapes should be successful"
      sli:
        events:
          # Error events: Failed scrapes
          errorQuery: |
            sum(rate(prometheus_target_scrapes_exceeded_sample_limit_total{
              job="prometheus-agent"
            }[5m])) +
            sum(rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{
              job="prometheus-agent"
            }[5m])) +
            sum(rate(prometheus_target_scrapes_sample_out_of_bounds_total{
              job="prometheus-agent"
            }[5m])) +
            sum(rate(prometheus_target_scrapes_sample_out_of_order_total{
              job="prometheus-agent"
            }[5m]))
          # Total events: All scrape attempts
          totalQuery: |
            sum(rate(prometheus_target_scrapes_total{
              job="prometheus-agent"
            }[5m]))
      alerting:
        name: PrometheusHighScrapeFailures
        labels:
          category: reliability
          severity: warning
          service: prometheus-agent
        annotations:
          summary: "Prometheus Agent scrape failure rate is above SLO threshold"
          description: |
            Prometheus Agent scrape success rate: {{ printf "%.2f" $value }}%
            SLO threshold: 99.5%
          runbook_url: "https://runbooks.example.com/prometheus/scrape-failures"

    # Remote write success rate SLO
    - name: "remote-write-success-rate"
      objective: 99.9
      description: "99.9% of remote write requests should be successful"
      sli:
        events:
          # Error events: Failed remote writes
          errorQuery: |
            sum(rate(prometheus_remote_storage_failed_samples_total{
              job="prometheus-agent"
            }[5m])) +
            sum(rate(prometheus_remote_storage_retried_samples_total{
              job="prometheus-agent"
            }[5m]))
          # Total events: All remote write samples
          totalQuery: |
            sum(rate(prometheus_remote_storage_samples_total{
              job="prometheus-agent"
            }[5m]))
      alerting:
        name: PrometheusRemoteWriteFailures
        labels:
          category: reliability
          severity: critical
          service: prometheus-agent
        annotations:
          summary: "Prometheus Agent remote write failure rate is above SLO threshold"
          description: |
            Remote write success rate: {{ printf "%.2f" $value }}%
            SLO threshold: 99.9%
          runbook_url: "https://runbooks.example.com/prometheus/remote-write-failures"

    # Metrics ingestion latency SLO
    - name: "ingestion-latency"
      objective: 99.0
      description: "99% of metrics should be ingested within 30 seconds"
      sli:
        events:
          # Error events: High ingestion delay
          errorQuery: |
            sum(rate(prometheus_tsdb_symbol_table_size_bytes{
              job="prometheus-agent"
            } < bool (time() - 30)))
          # Total events: All metrics ingested
          totalQuery: |
            sum(rate(prometheus_tsdb_symbol_table_size_bytes{
              job="prometheus-agent"
            } * 0 + 1))
      alerting:
        name: PrometheusHighIngestionLatency
        labels:
          category: latency
          severity: warning
          service: prometheus-agent
        annotations:
          summary: "Prometheus Agent ingestion latency is above SLO threshold"
          description: |
            Metrics ingestion latency SLO violation detected.
            {{ printf "%.2f" $value }}% of metrics ingested within 30s
            SLO threshold: 99%
          runbook_url: "https://runbooks.example.com/prometheus/ingestion-latency"

    # Target discovery SLO
    - name: "target-discovery"
      objective: 99.0
      description: "99% of service discovery should successfully find targets"
      sli:
        events:
          # Error events: Discovery failures
          errorQuery: |
            sum(rate(prometheus_sd_failed_configs_total{
              job="prometheus-agent"
            }[5m])) +
            sum(rate(prometheus_sd_file_read_errors_total{
              job="prometheus-agent"
            }[5m]))
          # Total events: All discovery operations
          totalQuery: |
            sum(rate(prometheus_sd_configs_total{
              job="prometheus-agent"
            }[5m]))
      alerting:
        name: PrometheusDiscoveryFailures
        labels:
          category: discovery
          severity: warning
          service: prometheus-agent
        annotations:
          summary: "Prometheus Agent service discovery failure rate is above SLO threshold"
          description: |
            Service discovery success rate: {{ printf "%.2f" $value }}%
            SLO threshold: 99%
          runbook_url: "https://runbooks.example.com/prometheus/discovery-failures"
